{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# importing tensorflow and keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "# Printing version of the TensorFlow\n",
    "\n",
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "Found 15480 files belonging to 34 classes.\n",
      "Using 12384 files for training.\n",
      "Found 15480 files belonging to 34 classes.\n",
      "Using 3096 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "import pathlib\n",
    "data_dir = pathlib.Path(\"images/Cyrillic\")\n",
    "image_count = len(list(data_dir.glob(\"*/*.png\")))\n",
    "print(image_count)\n",
    "batch_size = 32\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    subset=\"training\",\n",
    "    validation_split=0.2,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    subset=\"validation\",\n",
    "    validation_split=0.2,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=278x278>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_names = training_dataset.class_names\n",
    "images_test = list(data_dir.glob(\"*/*.png\"))\n",
    "PIL.Image.open(str(images_test[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in training_dataset:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general we should seek to make our input values small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the dataset for performance\n",
    "Let's make sure to use buffered prefetching so we can yield data from disk without having I/O become blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# def configure_for_performance(ds):\n",
    "#   ds = ds.cache()\n",
    "#   ds = ds.shuffle(buffer_size=1000)\n",
    "#   ds = ds.batch(batch_size)\n",
    "#   ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "#   return ds\n",
    "# training_dataset = configure_for_performance(training_dataset)\n",
    "# validation_dataset = configure_for_performance(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 34\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.0285 - loss: 3.5255 - val_accuracy: 0.0375 - val_loss: 3.5231\n",
      "Epoch 2/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0342 - loss: 3.5224 - val_accuracy: 0.0423 - val_loss: 3.5216\n",
      "Epoch 3/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.0330 - loss: 3.5210 - val_accuracy: 0.0423 - val_loss: 3.5210\n",
      "Epoch 4/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.0339 - loss: 3.5200 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 5/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0336 - loss: 3.5197 - val_accuracy: 0.0423 - val_loss: 3.5207\n",
      "Epoch 6/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0344 - loss: 3.5195 - val_accuracy: 0.0423 - val_loss: 3.5207\n",
      "Epoch 7/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0346 - loss: 3.5197 - val_accuracy: 0.0423 - val_loss: 3.5207\n",
      "Epoch 8/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0341 - loss: 3.5197 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 9/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0346 - loss: 3.5197 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 10/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0340 - loss: 3.5193 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 11/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0340 - loss: 3.5197 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 12/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0346 - loss: 3.5196 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 13/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0341 - loss: 3.5195 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 14/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0342 - loss: 3.5196 - val_accuracy: 0.0423 - val_loss: 3.5208\n",
      "Epoch 15/15\n",
      "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.0341 - loss: 3.5198 - val_accuracy: 0.0423 - val_loss: 3.5209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fec6a134310>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=validation_dataset,\n",
    "  epochs=15\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
